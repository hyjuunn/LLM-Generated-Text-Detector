{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8f7c310",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b58239ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_set.csv data loaded successfully.\n",
      "Total essays: 446345\n",
      "\n",
      "First 5 rows:\n",
      "                                                text  generated\n",
      "0  Car-free cities have become a subject of incre...          1\n",
      "1  Car Free Cities  Car-free cities, a concept ga...          1\n",
      "2    A Sustainable Urban Future  Car-free cities ...          1\n",
      "3    Pioneering Sustainable Urban Living  In an e...          1\n",
      "4    The Path to Sustainable Urban Living  In an ...          1\n"
     ]
    }
   ],
   "source": [
    "data_path = '../data/raw/'\n",
    "try:\n",
    "    df_train = pd.read_csv(f'{data_path}test_set.csv')\n",
    "    if 'generated' in df_train.columns:\n",
    "        df_train['generated'] = df_train['generated'].astype(int)\n",
    "    print(\"test_set.csv data loaded successfully.\")\n",
    "    print(f\"Total essays: {len(df_train)}\")\n",
    "    print(\"\\nFirst 5 rows:\")\n",
    "    print(df_train.head())\n",
    "except FileNotFoundError:\n",
    "    print(\"ERROR: train_essays.csv not found in data/raw/\")\n",
    "    print(\"Please make sure you have downloaded the data and placed it in the correct folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f19bacfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_whitespace(text):\n",
    "    \"\"\"\n",
    "    Function to normalize whitespace.\n",
    "    Replaces multiple whitespace chars with a single space and strips leading and trailing whilespace.\n",
    "    \"\"\"\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def remove_near_duplicates(df, column_name='text_cleaned'):\n",
    "    \"\"\"\n",
    "    Removes essays that are exact duplicates based on the specified column.\n",
    "    \"\"\"\n",
    "    initial_count = len(df)\n",
    "    # keep first\n",
    "    df = df.drop_duplicates(subset=[column_name], keep='first')\n",
    "    final_count = len(df)\n",
    "    print(f\"Removed {initial_count - final_count} exact duplicate essays.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0408a105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA CLEANING\n",
      "Whitespace Normalization\n",
      "Remove Dupe\n",
      "Removed 11830 exact duplicate essays.\n",
      "\n",
      "Cleaned DF head:\n",
      "                                        text_cleaned  generated\n",
      "0  Car-free cities have become a subject of incre...          1\n",
      "1  Car Free Cities Car-free cities, a concept gai...          1\n",
      "2  A Sustainable Urban Future Car-free cities are...          1\n",
      "3  Pioneering Sustainable Urban Living In an era ...          1\n",
      "4  The Path to Sustainable Urban Living In an age...          1\n"
     ]
    }
   ],
   "source": [
    "print(\"DATA CLEANING\")\n",
    "print(\"Whitespace Normalization\")\n",
    "df_train['text_cleaned'] = df_train['text'].apply(normalize_whitespace)\n",
    "print(\"Remove Dupe\")\n",
    "df_train = remove_near_duplicates(df_train, column_name='text_cleaned')\n",
    "if 'label' in df_train.columns:\n",
    "    df_train = df_train.rename(columns={'label': 'generated'})\n",
    "\n",
    "print(\"\\nCleaned DF head:\")\n",
    "print(df_train[['text_cleaned', 'generated']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f09bf83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Statistics (for Progress Report)\n",
      "\n",
      "Class Distribution (0 = Human, 1 = LLM):\n",
      "generated\n",
      "0    266658\n",
      "1    167857\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Essay Length Statistics (in words):\n",
      "count    434515.000000\n",
      "mean        393.084796\n",
      "std         168.716844\n",
      "min           0.000000\n",
      "25%         278.000000\n",
      "50%         362.000000\n",
      "75%         471.000000\n",
      "max        1668.000000\n",
      "Name: word_count, dtype: float64\n",
      "\n",
      "Example of a Cleaned Training Sample\n",
      "\n",
      "[Human Example (label = 0)]\n",
      "Phones Modern humans today are always on their phone. They are always on their phone more than 5 hours a day no stop .All they do is text back and forward and just have group Chats on social media. They even do it while driving. They are some really bad consequences when stuff happens when it comes to a phone. Some certain areas in the United States ban phones from class rooms just because of it. When people have phones, they know about certain apps that they have .Apps like Facebook Twitter Ins...\n",
      "\n",
      "[LLM Example (label = 1)]\n",
      "Car-free cities have become a subject of increasing interest and debate in recent years, as urban areas around the world grapple with the challenges of congestion, pollution, and limited resources. The concept of a car-free city involves creating urban environments where private automobiles are either significantly restricted or completely banned, with a focus on alternative transportation methods and sustainable urban planning. This essay explores the benefits, challenges, and potential solutio...\n"
     ]
    }
   ],
   "source": [
    "print(\"Data Statistics (for Progress Report)\")\n",
    "\n",
    "# print out class distribution\n",
    "class_counts = df_train['generated'].value_counts()\n",
    "print(\"\\nClass Distribution (0 = Human, 1 = LLM):\")\n",
    "print(class_counts)\n",
    "\n",
    "# get lengths\n",
    "df_train['word_count'] = df_train['text_cleaned'].apply(lambda x: len(x.split()))\n",
    "print(\"\\nEssay Length Statistics (in words):\")\n",
    "print(df_train['word_count'].describe())\n",
    "\n",
    "# example\n",
    "print(\"\\nExample of a Cleaned Training Sample\")\n",
    "try:\n",
    "    human_example = df_train[df_train['generated'] == 0].iloc[0]\n",
    "    print(f\"\\n[Human Example (label = 0)]\")\n",
    "    print(human_example['text_cleaned'][:500] + \"...\")\n",
    "\n",
    "    llm_example = df_train[df_train['generated'] == 1].iloc[0]\n",
    "    print(f\"\\n[LLM Example (label = 1)]\")\n",
    "    print(llm_example['text_cleaned'][:500] + \"...\")\n",
    "except IndexError:\n",
    "    print(\"\\nError: Could not find at least one example for both classes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43276dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400-Token Windowing (essay-level split first)\n",
      "Train essays: 347612\n",
      "Val essays:   86903\n",
      "\n",
      "Creating windows for TRAIN essays...\n",
      "Train windows: 398938\n",
      "Train class distribution (windows):\n",
      "generated\n",
      "0    259366\n",
      "1    139572\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Creating windows for VAL essays...\n",
      "Val windows: 100057\n",
      "Val class distribution (windows):\n",
      "generated\n",
      "0    65101\n",
      "1    34956\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"400-Token Windowing (essay-level split first)\")\n",
    "\n",
    "W_SIZE = 400\n",
    "STRIDE = 200\n",
    "\n",
    "def make_windows(df_in):\n",
    "    windowed_data = []\n",
    "    for _, row in df_in.iterrows():\n",
    "        text = row['text_cleaned']\n",
    "        label = row['generated']\n",
    "\n",
    "        tokens = text.split()\n",
    "\n",
    "        # use text as it is if smaller than window size\n",
    "        if len(tokens) <= W_SIZE:\n",
    "            windowed_data.append({\n",
    "                'text_window': text,\n",
    "                'generated': label\n",
    "            })\n",
    "        else:\n",
    "            for i in range(0, len(tokens) - W_SIZE + 1, STRIDE):\n",
    "                window_tokens = tokens[i : i + W_SIZE]\n",
    "                window_text = \" \".join(window_tokens)\n",
    "                windowed_data.append({\n",
    "                    'text_window': window_text,\n",
    "                    'generated': label\n",
    "                })\n",
    "\n",
    "    return pd.DataFrame(windowed_data)\n",
    "\n",
    "df_train_essays, df_val_essays = train_test_split(\n",
    "    df_train,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=df_train['generated']\n",
    ")\n",
    "\n",
    "print(f\"Train essays: {len(df_train_essays)}\")\n",
    "print(f\"Val essays:   {len(df_val_essays)}\")\n",
    "\n",
    "print(\"\\nCreating windows for TRAIN essays...\")\n",
    "df_train_windows = make_windows(df_train_essays)\n",
    "print(f\"Train windows: {len(df_train_windows)}\")\n",
    "print(\"Train class distribution (windows):\")\n",
    "print(df_train_windows['generated'].value_counts())\n",
    "\n",
    "print(\"\\nCreating windows for VAL essays...\")\n",
    "df_val_windows = make_windows(df_val_essays)\n",
    "print(f\"Val windows: {len(df_val_windows)}\")\n",
    "print(\"Val class distribution (windows):\")\n",
    "print(df_val_windows['generated'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5340f92e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ../data/processed\n"
     ]
    }
   ],
   "source": [
    "# save the result\n",
    "processed_path = '../data/processed'\n",
    "df_train_windows.to_csv(f'{processed_path}/new_train_windows.csv', index=False)\n",
    "df_val_windows.to_csv(f'{processed_path}/new_val_windows.csv', index=False)\n",
    "print(f\"Saved to {processed_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2e096d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
