{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4bb8d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training windows: 40796\n",
      "Validation windows: 10200\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# load data\n",
    "processed_path = '../data/processed'\n",
    "try:\n",
    "    df_train = pd.read_csv(f'{processed_path}/train_windows.csv')\n",
    "    df_val = pd.read_csv(f'{processed_path}/val_windows.csv')\n",
    "\n",
    "    # separate features (X) and target (y)\n",
    "    X_train = df_train['text_window']\n",
    "    y_train = df_train['generated']\n",
    "    \n",
    "    X_val = df_val['text_window']\n",
    "    y_val = df_val['generated']\n",
    "\n",
    "    print(f\"Training windows: {len(X_train)}\")\n",
    "    print(f\"Validation windows: {len(X_val)}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"ERROR: Processed data not found.\")\n",
    "    print(f\"Please make sure 'train_windows.csv' and 'val_windows.csv' are in {processed_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca43e133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train_pad: (40796, 400)\n",
      "Shape of X_val_pad: (10200, 400)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# tokenize and pad the text\n",
    "\n",
    "VOCAB_SIZE = 20000\n",
    "MAX_LEN = 400\n",
    "\n",
    "# word->number dict\n",
    "tokenizer = Tokenizer(num_words = VOCAB_SIZE)\n",
    "# only on the training\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# convert text to sequences of numbers\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_val_seq = tokenizer.texts_to_sequences(X_val)\n",
    "\n",
    "# pad seqs to be max len\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=MAX_LEN, padding='post')\n",
    "X_val_pad = pad_sequences(X_val_seq, maxlen=MAX_LEN, padding='post')\n",
    "\n",
    "print(f\"Shape of X_train_pad: {X_train_pad.shape}\")\n",
    "print(f\"Shape of X_val_pad: {X_val_pad.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de900655",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74fdb2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset/dataloader\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, seqeunces, labels):\n",
    "        self.sequences = torch.tensor(seqeunces, dtype=torch.long)\n",
    "        self.labels = torch.tensor(labels.values, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72bf0953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataloaders with batch size of 64\n",
      "\n",
      "DataLoaders Created\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "print(\"Creating dataloaders with batch size of 64\")\n",
    "\n",
    "train_dataset = TextDataset(X_train_pad, y_train)\n",
    "val_dataset = TextDataset(X_val_pad, y_val)\n",
    "\n",
    "# dataloaders\n",
    "train_loader = DataLoader(\n",
    "    dataset = train_dataset,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle = True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(\"\\nDataLoaders Created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208a69fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text-CNN model\n",
    "\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_filters, filter_sizes, output_dim, dropout):\n",
    "        super(TextCNN, self).__init__()\n",
    "        # embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        # Conv layers (parallel)\n",
    "        self.convos = nn.ModuleList([\n",
    "            nn.Conv1d(\n",
    "                in_channels=embed_dim,\n",
    "                out_channels=num_filters,\n",
    "                kernel_size=kern_size\n",
    "            ) for kern_size in filter_sizes\n",
    "        ])\n",
    "        # dropout and fcl\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # input size num_filters * len(filter_sizes) as it has to accept concatenation of 3 parallel conv/pool layers\n",
    "        self.fc1 = nn.Linear(num_filters * len(filter_sizes), 64)\n",
    "        self.fc2 = nn.Linear(64, output_dim)\n",
    "\n",
    "    # forward\n",
    "    # text -> [Batch_size, max_len]\n",
    "    def forward(self, text):\n",
    "        # embedding layer\n",
    "        embedded = self.embedding(text)\n",
    "        # [batch_size, max_len, embed_dim]\n",
    "\n",
    "        # reshape for conv\n",
    "        embedded = embedded.permute(0,2,1)\n",
    "        # [batch_size, embed_dim, max_len]\n",
    "\n",
    "        convos_res = [F.relu(conv(embedded)) for conv in self.convos]\n",
    "        # [batch_size, num_filters, (max_len - filter_size + 1)]\n",
    "\n",
    "        # global max pooling\n",
    "        pool_res = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in convos_res]\n",
    "        # [batch_size, num_filters]\n",
    "\n",
    "        # concatenate results\n",
    "        concat_res = torch.cat(pool_res, dim=1)\n",
    "        # [batch_size, num_filters*3]\n",
    "\n",
    "        # dropout and fc \n",
    "        drop_res = self.dropout(concat_res)\n",
    "        x1 = F.relu(self.fc1(drop_res))\n",
    "        x2 = self.fc2(x1)\n",
    "\n",
    "        return x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9347d656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized and moved to: cuda\n",
      "TextCNN(\n",
      "  (embedding): Embedding(20001, 100)\n",
      "  (convos): ModuleList(\n",
      "    (0): Conv1d(100, 64, kernel_size=(3,), stride=(1,))\n",
      "    (1): Conv1d(100, 64, kernel_size=(4,), stride=(1,))\n",
      "    (2): Conv1d(100, 64, kernel_size=(5,), stride=(1,))\n",
      "  )\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=192, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# init model & optim\n",
    "\n",
    "# hyperparams\n",
    "\n",
    "# VOCAB_SIZE is from the tokenizer\n",
    "# add 1 for the 0-padding token\n",
    "VOCAB_SIZE = tokenizer.num_words + 1\n",
    "EMBED_DIM = 100\n",
    "NUM_FILTERS = 64\n",
    "FILTER_SIZES = [3, 4, 5]\n",
    "OUTPUT_DIM = 2 # (Human=0, LLM=1)\n",
    "DROPOUT = 0.5\n",
    "\n",
    "# init model\n",
    "model = TextCNN(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    embed_dim=EMBED_DIM,\n",
    "    num_filters=NUM_FILTERS,\n",
    "    filter_sizes=FILTER_SIZES,\n",
    "    output_dim=OUTPUT_DIM,\n",
    "    dropout=DROPOUT\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# use GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "print(f\"Model initialized and moved to: {device}\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee648db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and evaluation\n",
    "\n",
    "from torchmetrics.classification import BinaryAccuracy, BinaryAUROC\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    # train for single epoch\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for texts, labels in dataloader:\n",
    "        texts = texts.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(texts)\n",
    "        loss = criterion(predictions, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate_epoch(model, dataloader, criterion, device):\n",
    "    # eval on validation set\n",
    "    print(\"Running evaluation...\")\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    # init metrics\n",
    "    # calculate accuracy and AUROC\n",
    "    acc_metric = BinaryAccuracy().to(device)\n",
    "    auroc_metric = BinaryAUROC().to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for texts, labels in dataloader:\n",
    "            texts = texts.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # forward pass\n",
    "            predictions = model(texts)\n",
    "            \n",
    "            # calculate loss\n",
    "            loss = criterion(predictions, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # update metrics\n",
    "            # get the probability of class 1 (LLM)\n",
    "            p1 = F.softmax(predictions, dim=1)[:, 1]\n",
    "            \n",
    "            # update the metrics with the batch results\n",
    "            acc_metric.update(p1, labels)\n",
    "            auroc_metric.update(p1, labels)\n",
    "    \n",
    "    # compute final metrics\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    # .compute() gets the final metric value from all batches\n",
    "    accuracy = acc_metric.compute()\n",
    "    auroc = auroc_metric.compute()\n",
    "        \n",
    "    return avg_loss, accuracy, auroc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d33739c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Training for 10 epochs ---\n",
      "Using device: cuda\n",
      "Running evaluation...\n",
      "\n",
      "Epoch: 01 | Time: 0m 8s\n",
      "\tTrain Loss: 0.290\n",
      "\t Val. Loss: 0.056 | Val. Acc: 98.50% | Val. AUROC: 0.9971\n",
      "\t^ New best model saved with AUROC: 0.9971\n",
      "Running evaluation...\n",
      "\n",
      "Epoch: 02 | Time: 0m 5s\n",
      "\tTrain Loss: 0.100\n",
      "\t Val. Loss: 0.034 | Val. Acc: 99.20% | Val. AUROC: 0.9980\n",
      "\t^ New best model saved with AUROC: 0.9980\n",
      "Running evaluation...\n",
      "\n",
      "Epoch: 03 | Time: 0m 5s\n",
      "\tTrain Loss: 0.066\n",
      "\t Val. Loss: 0.029 | Val. Acc: 99.25% | Val. AUROC: 0.9985\n",
      "\t^ New best model saved with AUROC: 0.9985\n",
      "Running evaluation...\n",
      "\n",
      "Epoch: 04 | Time: 0m 5s\n",
      "\tTrain Loss: 0.048\n",
      "\t Val. Loss: 0.023 | Val. Acc: 99.42% | Val. AUROC: 0.9989\n",
      "\t^ New best model saved with AUROC: 0.9989\n",
      "Running evaluation...\n",
      "\n",
      "Epoch: 05 | Time: 0m 5s\n",
      "\tTrain Loss: 0.033\n",
      "\t Val. Loss: 0.027 | Val. Acc: 99.33% | Val. AUROC: 0.9991\n",
      "\t^ New best model saved with AUROC: 0.9991\n",
      "Running evaluation...\n",
      "\n",
      "Epoch: 06 | Time: 0m 5s\n",
      "\tTrain Loss: 0.025\n",
      "\t Val. Loss: 0.022 | Val. Acc: 99.47% | Val. AUROC: 0.9992\n",
      "\t^ New best model saved with AUROC: 0.9992\n",
      "Running evaluation...\n",
      "\n",
      "Epoch: 07 | Time: 0m 5s\n",
      "\tTrain Loss: 0.017\n",
      "\t Val. Loss: 0.023 | Val. Acc: 99.40% | Val. AUROC: 0.9992\n",
      "\t^ New best model saved with AUROC: 0.9992\n",
      "Running evaluation...\n",
      "\n",
      "Epoch: 08 | Time: 0m 5s\n",
      "\tTrain Loss: 0.014\n",
      "\t Val. Loss: 0.028 | Val. Acc: 99.42% | Val. AUROC: 0.9991\n",
      "\tNo improvement. Patience: 1/2\n",
      "Running evaluation...\n",
      "\n",
      "Epoch: 09 | Time: 0m 5s\n",
      "\tTrain Loss: 0.012\n",
      "\t Val. Loss: 0.029 | Val. Acc: 99.40% | Val. AUROC: 0.9991\n",
      "\tNo improvement. Patience: 2/2\n",
      "\n",
      "--- Early stopping triggered after 9 epochs ---\n",
      "\n",
      "--- Training Complete ---\n",
      "Best Validation AUROC achieved: 0.9992\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "\n",
    "import time\n",
    "\n",
    "N_EPOCHS = 10\n",
    "best_val_auroc = 0.0\n",
    "PATIENCE = 2\n",
    "epochs_no_improve = 0\n",
    "\n",
    "print(f\"--- Starting Training for {N_EPOCHS} epochs ---\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    val_loss, val_acc, val_auroc = evaluate_epoch(model, val_loader, criterion, device)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = divmod(end_time - start_time, 60)\n",
    "    \n",
    "    print(f\"\\nEpoch: {epoch+1:02} | Time: {epoch_mins:.0f}m {epoch_secs:.0f}s\")\n",
    "    print(f\"\\tTrain Loss: {train_loss:.3f}\")\n",
    "    print(f\"\\t Val. Loss: {val_loss:.3f} | Val. Acc: {val_acc.item()*100:.2f}% | Val. AUROC: {val_auroc.item():.4f}\")\n",
    "    \n",
    "    # early stopping\n",
    "    if val_auroc > best_val_auroc:\n",
    "        best_val_auroc = val_auroc\n",
    "        epochs_no_improve = 0\n",
    "        torch.save(model.state_dict(), 'best_cnn_model.pth')\n",
    "        print(f\"\\t^ New best model saved with AUROC: {best_val_auroc.item():.4f}\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        print(f\"\\tNo improvement. Patience: {epochs_no_improve}/{PATIENCE}\")\n",
    "    \n",
    "    if epochs_no_improve == PATIENCE:\n",
    "        print(f\"\\n--- Early stopping triggered after {epoch+1} epochs ---\")\n",
    "        break\n",
    "\n",
    "print(\"\\n--- Training Complete ---\")\n",
    "print(f\"Best Validation AUROC achieved: {best_val_auroc.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0f34e18e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Test: Manual Qualitative Test ---\n",
      "Successfully loaded 'best_cnn_model.pth'\n",
      "Predicting on custom sentences...\n",
      "\n",
      "Prediction: Human (0) (Confidence: 99.30%)\n",
      "Text: 'In this lab, I had to use a technique called chromatography. We used paper chromatography to separate metal ions. Generally, chromatography is a simple way of identifying an unknown mixture of chemicals or compounds, which are in liquid or gas form, by dissolving mixtures in a fluid called the mobile phase that carries through the stationary phase. The mobile phase can be a proper liquid solvent or mixture of solvents, while the stationary phase is a solid or liquid phase that is fixed in a place in the experiment. In our lab, we used a specific chromatography called paper chromatography. What I learned during this lab, in general, is that paper chromatography works based on capillary action. Capillary action is the tendency of liquid to rise in thin tubes or to be brought into small openings; It happens because there are adhesive forces between the molecules of the solvent. In paper chromatography, the liquid rises up through the paper, which is the stationary medium, since there are small holes in between the paper fibres. The main point of chromatography, however, is that it uses the difference in solubility of substances in a solvent. Solubility basically means how much of a particular substance can dissolve in a specific solvent, and the difference of it creates the various substances to leave solution at varying points as the solvent rises up the stationary phase. In this case, I noticed that the substance will travel more if it is more soluble. Also, absorption takes part in creating separation. Higher absorption to the stationary phase will slow down the molecule that moves through the column.'\n",
      "\n",
      "Prediction: LLM (1) (Confidence: 78.63%)\n",
      "Text: 'In this lab, we used a technique called paper chromatography to separate and identify metal ions. Chromatography is a method used to analyze mixtures of chemicals by separating them based on how they move through two phases: a mobile phase (a liquid or gas that moves) and a stationary phase (a solid or liquid fixed in place). In our experiment, the stationary phase was the paper, and the mobile phase was the solvent. Paper chromatography works mainly due to capillary action, which is the ability of a liquid to flow through narrow spaces without external force. This happens because of adhesive forces between the liquid molecules and the paper fibers. As the solvent rises up the paper, it carries the dissolved substances with it. The key principle of chromatography is that different substances have different solubilities in the solvent and different levels of absorption (or attraction) to the stationary phase. Substances that are more soluble in the solvent travel farther up the paper, while substances that are more strongly absorbed by the paper move more slowly. From this lab, I learned that paper chromatography separates substances based on their solubility and their interaction with the stationary phase.'\n",
      "\n",
      "Prediction: Human (0) (Confidence: 61.00%)\n",
      "Text: 'In this lab, I learned about how the system shifts to alleviate stress which allows the system to reach a new equilibrium. When we add a chemical species, the equilibrium system will remove the added species by shifting to the other side while the equilibrium system will replace the species by shifting if we remove some of the chemical species by the process of neutralizing, complex ion formation, or producing precipitate. This is due to a change in concentration; when concentration increases, the reaction rate increases the frequency of collision while it does opposite when concentration decreases. In the addition of a chemical species, ions considered as spectator ions did not affect the shift. Furthermore, temperature change would affect frequency of collision and fraction of successful collision which would affect the reaction rate. When a system is heated, the system will shift toward the endothermic direction in order to remove added energy while the system will shift toward the exothermic side to replace lost energy when the system is cooled. In general, the endothermic side is more sensitive to the temperature change due to its high activation energy. When we were determining the effects of the stress, we had to depend on our observations. We basically kept track of the colour changes or formation of precipitation to determine the shifts in the system. The colours became visible when we diluted the solution. In addition, I learned that kinetics can go along with Le Chatelier’s Principle. When the system is stressed, the forward and reverse reaction rates would both increase or decrease. However, they would have different magnitudes. For example, when we removed the species, the concentration of the reactants had a lower concentration so that the frequency of collision decreased; this decreased the forward rate more than the reverse rate so that it caused a left shift to occur. '\n",
      "\n",
      "Prediction: LLM (1) (Confidence: 99.95%)\n",
      "Text: 'In this lab, I learned how a chemical system responds to stress in order to re-establish equilibrium, according to Le Chatelier's Principle. When a chemical species is added, the system shifts to the opposite side to reduce the increase. Conversely, when a species is removed—through neutralization, complex ion formation, or precipitation—the system shifts to replace what was removed. This happens because changes in concentration affect the reaction rate by altering the frequency of collisions between particles. Spectator ions, however, do not influence the direction of the shift. I also learned how temperature affects equilibrium. A temperature increase causes the system to shift toward the endothermic direction to absorb excess energy, while cooling shifts the equilibrium toward the exothermic direction to release energy. The endothermic side is generally more sensitive to temperature changes because it has a higher activation energy. To determine how the equilibrium shifted, we relied on observations of color changes or the formation of precipitates. Diluting the solution often made these color changes more visible. Additionally, I learned that reaction kinetics work alongside equilibrium: when a system is stressed, both the forward and reverse reaction rates change, but not equally. For example, removing a reactant decreases its concentration and collision frequency, reducing the forward rate more than the reverse rate. This results in a shift toward the left, or the reactant side.'\n",
      "\n",
      "Prediction: LLM (1) (Confidence: 83.53%)\n",
      "Text: 'Probability is a branch of mathematics that measures how likely an event is to happen. It provides a way to quantify uncertainty by assigning a number between 0 and 1 to events, where 0 means the event is impossible and 1 means it is certain. For example, when flipping a fair coin, the probability of getting heads is 0.5 because it is equally likely to land on heads or tails. Probability helps us make predictions, analyze patterns, and understand random processes in everyday life, science, and engineering.'\n",
      "\n",
      "Prediction: LLM (1) (Confidence: 72.97%)\n",
      "Text: 'But what makes our map truly functional are its important functionalities and features that have been carefully integrated to make your navigation experience smoother. For instance, when users are using the search bar, the search bar provides 4 different autofill options so that users can simply select without typing the whole word on the search bar. Also, when we place the cursor over a specific button, it shows a small message or description about what it will show.'\n",
      "\n",
      "Prediction: LLM (1) (Confidence: 99.35%)\n",
      "Text: 'What makes our map truly effective are the carefully designed features that enhance the overall navigation experience. For example, the search bar offers four smart autofill suggestions, allowing users to quickly select a result without typing the entire word. Additionally, when users hover over a button, a small tooltip appears with a brief description of its function, making the interface more intuitive and user-friendly.'\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "print(\"--- Test: Manual Qualitative Test ---\")\n",
    "\n",
    "# load model\n",
    "# re-initialize the model architecture\n",
    "VOCAB_SIZE = tokenizer.num_words + 1 \n",
    "EMBED_DIM = 100\n",
    "NUM_FILTERS = 64\n",
    "FILTER_SIZES = [3, 4, 5]\n",
    "OUTPUT_DIM = 2\n",
    "DROPOUT = 0.5\n",
    "\n",
    "# create a new instance\n",
    "test_model = TextCNN(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    embed_dim=EMBED_DIM,\n",
    "    num_filters=NUM_FILTERS,\n",
    "    filter_sizes=FILTER_SIZES,\n",
    "    output_dim=OUTPUT_DIM,\n",
    "    dropout=DROPOUT\n",
    ")\n",
    "\n",
    "# load the saved weights\n",
    "test_model.load_state_dict(torch.load('best_cnn_model.pth'))\n",
    "test_model.to(device)\n",
    "test_model.eval()\n",
    "print(\"Successfully loaded 'best_cnn_model.pth'\")\n",
    "\n",
    "# examples\n",
    "my_examples = [\n",
    "    # Example 1: my writing\n",
    "    \"In this lab, I had to use a technique called chromatography. We used paper chromatography to separate metal ions. Generally, chromatography is a simple way of identifying an unknown mixture of chemicals or compounds, which are in liquid or gas form, by dissolving mixtures in a fluid called the mobile phase that carries through the stationary phase. The mobile phase can be a proper liquid solvent or mixture of solvents, while the stationary phase is a solid or liquid phase that is fixed in a place in the experiment. In our lab, we used a specific chromatography called paper chromatography. What I learned during this lab, in general, is that paper chromatography works based on capillary action. Capillary action is the tendency of liquid to rise in thin tubes or to be brought into small openings; It happens because there are adhesive forces between the molecules of the solvent. In paper chromatography, the liquid rises up through the paper, which is the stationary medium, since there are small holes in between the paper fibres. The main point of chromatography, however, is that it uses the difference in solubility of substances in a solvent. Solubility basically means how much of a particular substance can dissolve in a specific solvent, and the difference of it creates the various substances to leave solution at varying points as the solvent rises up the stationary phase. In this case, I noticed that the substance will travel more if it is more soluble. Also, absorption takes part in creating separation. Higher absorption to the stationary phase will slow down the molecule that moves through the column.\",\n",
    "    \n",
    "    # Example 2: llm writing\n",
    "    \"In this lab, we used a technique called paper chromatography to separate and identify metal ions. Chromatography is a method used to analyze mixtures of chemicals by separating them based on how they move through two phases: a mobile phase (a liquid or gas that moves) and a stationary phase (a solid or liquid fixed in place). In our experiment, the stationary phase was the paper, and the mobile phase was the solvent. Paper chromatography works mainly due to capillary action, which is the ability of a liquid to flow through narrow spaces without external force. This happens because of adhesive forces between the liquid molecules and the paper fibers. As the solvent rises up the paper, it carries the dissolved substances with it. The key principle of chromatography is that different substances have different solubilities in the solvent and different levels of absorption (or attraction) to the stationary phase. Substances that are more soluble in the solvent travel farther up the paper, while substances that are more strongly absorbed by the paper move more slowly. From this lab, I learned that paper chromatography separates substances based on their solubility and their interaction with the stationary phase.\",\n",
    "    \n",
    "    # Example 3: my writing\n",
    "    \"In this lab, I learned about how the system shifts to alleviate stress which allows the system to reach a new equilibrium. When we add a chemical species, the equilibrium system will remove the added species by shifting to the other side while the equilibrium system will replace the species by shifting if we remove some of the chemical species by the process of neutralizing, complex ion formation, or producing precipitate. This is due to a change in concentration; when concentration increases, the reaction rate increases the frequency of collision while it does opposite when concentration decreases. In the addition of a chemical species, ions considered as spectator ions did not affect the shift. Furthermore, temperature change would affect frequency of collision and fraction of successful collision which would affect the reaction rate. When a system is heated, the system will shift toward the endothermic direction in order to remove added energy while the system will shift toward the exothermic side to replace lost energy when the system is cooled. In general, the endothermic side is more sensitive to the temperature change due to its high activation energy. When we were determining the effects of the stress, we had to depend on our observations. We basically kept track of the colour changes or formation of precipitation to determine the shifts in the system. The colours became visible when we diluted the solution. In addition, I learned that kinetics can go along with Le Chatelier’s Principle. When the system is stressed, the forward and reverse reaction rates would both increase or decrease. However, they would have different magnitudes. For example, when we removed the species, the concentration of the reactants had a lower concentration so that the frequency of collision decreased; this decreased the forward rate more than the reverse rate so that it caused a left shift to occur. \",\n",
    "\n",
    "    # Example 4: llm writing\n",
    "    \"In this lab, I learned how a chemical system responds to stress in order to re-establish equilibrium, according to Le Chatelier's Principle. When a chemical species is added, the system shifts to the opposite side to reduce the increase. Conversely, when a species is removed—through neutralization, complex ion formation, or precipitation—the system shifts to replace what was removed. This happens because changes in concentration affect the reaction rate by altering the frequency of collisions between particles. Spectator ions, however, do not influence the direction of the shift. I also learned how temperature affects equilibrium. A temperature increase causes the system to shift toward the endothermic direction to absorb excess energy, while cooling shifts the equilibrium toward the exothermic direction to release energy. The endothermic side is generally more sensitive to temperature changes because it has a higher activation energy. To determine how the equilibrium shifted, we relied on observations of color changes or the formation of precipitates. Diluting the solution often made these color changes more visible. Additionally, I learned that reaction kinetics work alongside equilibrium: when a system is stressed, both the forward and reverse reaction rates change, but not equally. For example, removing a reactant decreases its concentration and collision frequency, reducing the forward rate more than the reverse rate. This results in a shift toward the left, or the reactant side.\",\n",
    "\n",
    "    # Example 5: llm writing\n",
    "    \"Probability is a branch of mathematics that measures how likely an event is to happen. It provides a way to quantify uncertainty by assigning a number between 0 and 1 to events, where 0 means the event is impossible and 1 means it is certain. For example, when flipping a fair coin, the probability of getting heads is 0.5 because it is equally likely to land on heads or tails. Probability helps us make predictions, analyze patterns, and understand random processes in everyday life, science, and engineering.\",\n",
    "\n",
    "    # Example 6: my writing\n",
    "    \"But what makes our map truly functional are its important functionalities and features that have been carefully integrated to make your navigation experience smoother. For instance, when users are using the search bar, the search bar provides 4 different autofill options so that users can simply select without typing the whole word on the search bar. Also, when we place the cursor over a specific button, it shows a small message or description about what it will show.\",\n",
    "\n",
    "    # Example 7: llm writing\n",
    "    \"What makes our map truly effective are the carefully designed features that enhance the overall navigation experience. For example, the search bar offers four smart autofill suggestions, allowing users to quickly select a result without typing the entire word. Additionally, when users hover over a button, a small tooltip appears with a brief description of its function, making the interface more intuitive and user-friendly.\"\n",
    "]\n",
    "\n",
    "# process and predict\n",
    "print(\"Predicting on custom sentences...\")\n",
    "with torch.no_grad():\n",
    "    for text in my_examples:\n",
    "        #tokenize (use same tokenizer)\n",
    "        seq = tokenizer.texts_to_sequences([text])\n",
    "        \n",
    "        # pad\n",
    "        padded_seq = pad_sequences(seq, maxlen=MAX_LEN, padding='post')\n",
    "        \n",
    "        # convert to tensor\n",
    "        tensor = torch.tensor(padded_seq, dtype=torch.long).to(device)\n",
    "        \n",
    "        # get prediction\n",
    "        prediction = test_model(tensor)\n",
    "        \n",
    "        # probs and class\n",
    "        probs = F.softmax(prediction, dim=1)\n",
    "        pred_class = torch.argmax(probs, dim=1).item()\n",
    "        confidence = probs[0][pred_class].item() * 100\n",
    "        \n",
    "        label = \"LLM (1)\" if pred_class == 1 else \"Human (0)\"\n",
    "        \n",
    "        print(f\"\\nPrediction: {label} (Confidence: {confidence:.2f}%)\")\n",
    "        print(f\"Text: '{text}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f423b98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
