{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25095da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Training Set Size: 44868\n",
      "Sample Test Set Size: 487235\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# training data\n",
    "train_path = \"../data/raw/train_v2_drcat_02.csv\"\n",
    "df_train = pd.read_csv(train_path)\n",
    "\n",
    "# test data\n",
    "test_path = \"../data/ood/ood_data.csv\"\n",
    "df_test_candidate = pd.read_csv(test_path)\n",
    "\n",
    "print(f\"Original Training Set Size: {len(df_train)}\")\n",
    "print(f\"Sample Test Set Size: {len(df_test_candidate)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89c7fdb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Data Analysis ---\n",
      "Overlapping essays removed: 40890\n",
      "Clean, valid 'New Data' essays remaining: 446345\n",
      "SUCCESS: Enough unique data present.\n",
      "Saved clean test set to data/raw\n"
     ]
    }
   ],
   "source": [
    "# remove overlaps\n",
    "\n",
    "clean_test_df = df_test_candidate[~df_test_candidate['text'].isin(df_train['text'])]\n",
    "\n",
    "num_removed = len(df_test_candidate) - len(clean_test_df)\n",
    "\n",
    "print(f\"--- Data Analysis ---\")\n",
    "print(f\"Overlapping essays removed: {num_removed}\")\n",
    "print(f\"Clean, valid 'New Data' essays remaining: {len(clean_test_df)}\")\n",
    "\n",
    "if len(clean_test_df) > 0:\n",
    "    print(\"SUCCESS: Enough unique data present.\")\n",
    "    clean_test_df.to_csv('../data/raw/test_set.csv', index=False)\n",
    "    print(\"Saved clean test set to data/raw\")\n",
    "else:\n",
    "    print(\"Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bb58e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac173d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = 20000 + 1\n",
    "EMBED_DIM = 100\n",
    "NUM_FILTERS = 64\n",
    "FILTER_SIZES = [3, 4, 5]\n",
    "OUTPUT_DIM = 2\n",
    "DROPOUT = 0.5\n",
    "MAX_LEN = 400\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_filters, filter_sizes, output_dim, dropout):\n",
    "        super(TextCNN, self).__init__()\n",
    "        \n",
    "        # embedding Layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        \n",
    "        # Conv Layers (parallel)\n",
    "        self.convos = nn.ModuleList([\n",
    "            nn.Conv1d(\n",
    "                in_channels=embed_dim,\n",
    "                out_channels=num_filters,\n",
    "                kernel_size=kern_size\n",
    "            ) for kern_size in filter_sizes\n",
    "        ])\n",
    "        \n",
    "        # dropout and fc\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # input size num filters * len(filter_sizes)\n",
    "        self.fc1 = nn.Linear(num_filters * len(filter_sizes), 64)\n",
    "        self.fc2 = nn.Linear(64, output_dim)\n",
    "\n",
    "    def forward(self, text):\n",
    "        # embedding Layer\n",
    "        embedded = self.embedding(text)\n",
    "        # reshape for conv [batch_size, embed_dim, max_len]\n",
    "        embedded = embedded.permute(0, 2, 1)\n",
    "        \n",
    "        convos_res = [F.relu(conv(embedded)) for conv in self.convos]\n",
    "        \n",
    "        # global max pooling\n",
    "        pool_res = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in convos_res]\n",
    "        \n",
    "        # concatenate results\n",
    "        concat_res = torch.cat(pool_res, dim=1)\n",
    "        \n",
    "        # dropout and fc\n",
    "        drop_res = self.dropout(concat_res)\n",
    "        x1 = F.relu(self.fc1(drop_res))\n",
    "        x2 = self.fc2(x1)\n",
    "        \n",
    "        return x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86827bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer fitted. Vocab size: 82222\n"
     ]
    }
   ],
   "source": [
    "data_path = '../data/raw/'\n",
    "\n",
    "def normalize_whitespace(text):\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "try:\n",
    "    # load original training data\n",
    "    df_train_orig = pd.read_csv(f'{data_path}train_v2_drcat_02.csv')\n",
    "    \n",
    "    # clean\n",
    "    df_train_orig['text_cleaned'] = df_train_orig['text'].apply(normalize_whitespace)\n",
    "    \n",
    "    # fit tokenizer\n",
    "    tokenizer = Tokenizer(num_words=VOCAB_SIZE - 1)\n",
    "    tokenizer.fit_on_texts(df_train_orig['text_cleaned'])\n",
    "    \n",
    "    print(f\"Tokenizer fitted. Vocab size: {len(tokenizer.word_index)}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"ERROR: Training data not found. Cannot rebuild tokenizer.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "028de498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Processing Test Set ---\n",
      "Created 512472 windows from 446345 essays.\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Processing Test Set ---\")\n",
    "# load test set\n",
    "df_test = pd.read_csv(f'{data_path}test_set.csv')\n",
    "\n",
    "# clean\n",
    "df_test['text_cleaned'] = df_test['text'].apply(normalize_whitespace)\n",
    "\n",
    "# windowing\n",
    "W_SIZE = 400\n",
    "STRIDE = 200\n",
    "test_windowed_data = []\n",
    "\n",
    "for idx, row in df_test.iterrows():\n",
    "    text = row['text_cleaned']\n",
    "    label = row['generated'] if 'generated' in row else -1\n",
    "    \n",
    "    tokens = text.split()\n",
    "    \n",
    "    if len(tokens) <= W_SIZE:\n",
    "        test_windowed_data.append({\n",
    "            'essay_id': idx, \n",
    "            'text_window': text, \n",
    "            'generated': label\n",
    "        })\n",
    "    else:\n",
    "        for i in range(0, len(tokens) - W_SIZE + 1, STRIDE):\n",
    "            window_tokens = tokens[i : i + W_SIZE]\n",
    "            window_text = \" \".join(window_tokens)\n",
    "            test_windowed_data.append({\n",
    "                'essay_id': idx, \n",
    "                'text_window': window_text, \n",
    "                'generated': label\n",
    "            })\n",
    "\n",
    "df_test_windows = pd.DataFrame(test_windowed_data)\n",
    "print(f\"Created {len(df_test_windows)} windows from {len(df_test)} essays.\")\n",
    "\n",
    "# tokenize and pad\n",
    "X_test_seq = tokenizer.texts_to_sequences(df_test_windows['text_window'])\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test_pad, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dffc2a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running Prediction ---\n",
      "Successfully loaded 'best_cnn_model.pth'\n",
      "\n",
      "--- Final Aggregation ---\n",
      "Total Essays: 446345\n",
      "Abstained: 69387 (15.5%)\n",
      "Decided: 376958\n",
      "Accuracy (on decisions): 75.65%\n",
      "\n",
      "Sample Results:\n",
      "   chunk_score  prediction  generated\n",
      "0     0.621636          -1        1.0\n",
      "1     0.999188           1        1.0\n",
      "2     0.949429           1        1.0\n",
      "3     0.978913           1        1.0\n",
      "4     0.952428           1        1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Running Prediction ---\")\n",
    "\n",
    "test_model = TextCNN(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    embed_dim=EMBED_DIM,\n",
    "    num_filters=NUM_FILTERS,\n",
    "    filter_sizes=FILTER_SIZES,\n",
    "    output_dim=OUTPUT_DIM,\n",
    "    dropout=DROPOUT\n",
    ")\n",
    "\n",
    "try:\n",
    "    test_model.load_state_dict(torch.load('best_cnn_model.pth', map_location=device))\n",
    "    test_model.to(device)\n",
    "    test_model.eval()\n",
    "    print(\"Successfully loaded 'best_cnn_model.pth'\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Weight file not found...\")\n",
    "\n",
    "# inference\n",
    "all_probs = []\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(X_test_tensor), BATCH_SIZE):\n",
    "        batch = X_test_tensor[i : i + BATCH_SIZE].to(device)\n",
    "        predictions = test_model(batch)\n",
    "        \n",
    "        # get probs\n",
    "        probs = F.softmax(predictions, dim=1)[:, 1]\n",
    "        all_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "df_test_windows['chunk_score'] = all_probs\n",
    "\n",
    "# aggregate by essay\n",
    "print(\"\\n--- Final Aggregation ---\")\n",
    "final_results = df_test_windows.groupby('essay_id').agg({\n",
    "    'chunk_score': 'mean',\n",
    "    'generated': 'first'\n",
    "}).reset_index()\n",
    "\n",
    "# abstain setup\n",
    "LOW_CONF = 0.20\n",
    "HIGH_CONF = 0.80\n",
    "\n",
    "def get_decision(score):\n",
    "    if score < LOW_CONF: return 0\n",
    "    elif score > HIGH_CONF: return 1\n",
    "    else: return -1\n",
    "\n",
    "final_results['prediction'] = final_results['chunk_score'].apply(get_decision)\n",
    "\n",
    "# metrics\n",
    "total = len(final_results)\n",
    "abstained = len(final_results[final_results['prediction'] == -1])\n",
    "decided = final_results[final_results['prediction'] != -1]\n",
    "\n",
    "accuracy = 0\n",
    "if len(decided) > 0:\n",
    "    correct = len(decided[decided['prediction'] == decided['generated']])\n",
    "    accuracy = correct / len(decided)\n",
    "\n",
    "print(f\"Total Essays: {total}\")\n",
    "print(f\"Abstained: {abstained} ({abstained/total:.1%})\")\n",
    "print(f\"Decided: {len(decided)}\")\n",
    "print(f\"Accuracy (on decisions): {accuracy:.2%}\")\n",
    "\n",
    "print(\"\\nSample Results:\")\n",
    "print(final_results[['chunk_score', 'prediction', 'generated']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24bc497",
   "metadata": {},
   "source": [
    "The 75.65% acccuracy on the new large test set is primarily due to the shift in distribution. The initial text-CNN model was trained on a relatively small DAIGT V2 dataset and seemed like it overfitted to dataset specific artifacts such as vocab, formats..etc.\n",
    "Now, it requires broader training data for improvements. (I will use this large new dataset to re-train the model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
